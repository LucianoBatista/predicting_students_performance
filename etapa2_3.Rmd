---
title: "Etapa 2 e 3"
---

<style>
body {
text-align: justify}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE, dpi = 300, fig.align = "center", fig.width = 10, fig.height = 7)

```

As etapas 2 e 3 foram destinadas para modelagem. O problema escolhido foi o de predizer o desempenho dos estudantes nas avaliações finais e além disso, o candidato deveria avaliar a qualidade e precisão do modelo.

No entanto algumas ressalvas precisam ser feitas: em relação a task pedida, a documentação da OU (Open University) e a integridade dos dados.

As avaliações finais no dataset correspondem a classe *"Exam"*, presente na variável `assessment_type`. No entanto, possuímos muitos valores missing nesta variável e sua importância para composição da nota final é muitas vezes dúbia. Isso por que encontramos estudantes que pontuaram 0 no `Exam` e ainda assim foram aprovados, assim como estudantes que não fizeram o teste e foram aprovados do mesmo jeito.

Uma explicação é que as avaliações finais servem para compor sua nota final, valor esse que de fato vai indicar se o estudante foi aprovado ou não no curso. E essa composição não é dita na documentação dos dados, e é pouco provável de ser encontrada apenas pela análise dos dados atuais.

Tem mais um ponto que precisa ser comentado, ao pensar em utilizar as notas individuais em cada avaliação encontrei outro problema. Alguns cursos zeraram os pesos de cada teste, gerando assim mais incosistências em relação a correlação de quem foi aprovado ou não e seu desempenho. 

# Libs

Abaixo você encontra os pacotes utilizados para esta etapa do case.

```{r}
library(tidyverse)
library(skimr)
library(data.table)
library(tidytable)
library(data.table)
library(lubridate)
library(tidytext)
library(ggridges)
library(sf)
library(geofacet)
library(rlang)
library(plotly)
library(tidymodels)
library(DT)
library(kableExtra)
```


# Procedimento adotado

Diante ao que foi dito, o procedimento adotado aqui foi o de retirar os valores missings da variável `assessment_type` e trabalhar com as informações que sobrarem. Sendo que essa é nossa variável target, não faria sentido utilizar nenhum método de imputação nela.

Para obter o dataset para modelagem precisamos realizar uma série de joins entre as tableas disponíveis, para que cada observação dos dados fosse composta por um estudante de um determinado período e curso diferente.

```{r}
# obtendo o dataset para modelagem
# primeira parte: dados apenas sobre as avaliações finais (Exam) 
final_score_exam_tbl <- student_info %>% 
  # coletando pesos e ids das avaliações 
  left_join(assessments, by = c("code_module", "code_presentation")) %>% 
  select(-gender:-disability, -date) %>% 
  # coletando as notas individuais dos estudantes
  left_join(student_assessment, by = c("id_student", "id_assessment")) %>% 
  mutate(weighted = (weight/100) * score) %>% 
  filter(assessment_type == "Exam") %>% 
  drop_na(score) %>% 
  group_by(code_module, code_presentation, id_student, final_result) %>% 
  summarise(final_score_exam = sum(weighted)) %>% 
  ungroup()

# segunda parte: join dos dados sobre as avaliações finais com o dataset base (student_info)
exam_info_tbl <- student_info %>% 
  left_join(final_score_exam_tbl, by = c("code_module", "code_presentation", "id_student")) %>% 
  select(-final_result.y) %>% 
  rename(final_result = final_result.x) %>% 
  drop_na(final_score_exam)

```


```{r}
kbl(exam_info_tbl %>% head()) %>%
  kable_paper("striped") %>%
  scroll_box(width = "100%")

```
<p style="margin-bottom:40px"></p>

Algumas variáveis não fazem sentido serem adicionadas para treino do modelo, pois não são informações previamente disponíveis para cada estudante, ou seja, elas aconteceram apenas no futuro e portanto não tem como serem usadas como preditoras. Por conta disso, iremos dropar algumas features.

```{r}
exam_info_v2_tbl <- exam_info_tbl %>%
  select(-studied_credits, -final_result)

```

Meu intuito inicial era realizar a previsão do desempenho por curso, porém, como temos poucos dados fica inviável tal abordagem. 

Como nosso dataset é predominantemente categórico, é super interessante observar como está a distribuição das diferentes classes. Para isso eu criei uma função que nos permite visualizar todas as features com apenas um comando.

```{r}
plot_all_full_data <- function(name = "region") {
  
  exam_info_v2_tbl %>% 
    count(.data[[name]]) %>% 
    mutate(prop = n/sum(n)) %>% 
    ggplot(aes(y = fct_reorder(.data[[name]], prop),
               x = prop)) +
    geom_segment(aes(xend = 0, yend = .data[[name]]),
                 show.legend = F) +
    geom_point(aes(size = prop),
               show.legend = F) +
    geom_label(aes(label = prop %>% scales::number(accuracy = .01), size = prop*50), 
               fill = "white", 
               hjust = "inward",
               show.legend = F) +
    theme_minimal() +
    labs(
      y = name
    )
}

names <- names(exam_info_v2_tbl %>% select(where(is.character)))
all_plots <- map(names, .f = plot_all_full_data)
cowplot::plot_grid(plotlist = all_plots)

```

Vimos aqui que algumas classes são bem desiquilibradas e isso pode ser um problema para nossa previsão. Por enquanto seguiremos desta forma para criação de um modelo baseline.

Além disso, a variável `imd_band` possui alguns valores missing. Por enquanto a mesma não será considerada.

```{r}
exam_info_final_tbl <- exam_info_v2_tbl %>% select(-imd_band)

kbl(exam_info_final_tbl %>% head()) %>%
  kable_paper("striped") %>%
  scroll_box(width = "100%")

```

Sendo assim, nosso dataset que será utilizado para as previsões é o `exam_info_final_tbl`.


# Split de treino e teste

Precisamos dividir nossos dados entre treino e teste. Um problema que pode ocorrer aqui é das classes ficarem com distribuições diferentes, ou ainda classes de pouca ocorrência ficarem fora de algum conjunto de dado. A função `initial_split()` possui uma forma de auxiliar nessa tarefa, o parâmetro `strata`, este serve para indicar qual variável irá manter a proporção de classes na hora do split.

Nesse caso eu optei pela variável `highest_education` por conter algumas classes com pouquísimos valores.

```{r}
set.seed(1234)
splits <- initial_split(prop = .7, strata = "highest_education", data = exam_info_final_tbl)

train <- training(splits)
test <- testing(splits)

```

Vamos checar novamente as distribuições.

```{r}
plot_all_full_train <- function(name = "region") {
  
  train %>% 
    count(.data[[name]]) %>% 
    mutate(prop = n/sum(n)) %>% 
    ggplot(aes(y = fct_reorder(.data[[name]], prop),
               x = prop)) +
    geom_segment(aes(xend = 0, yend = .data[[name]]),
                 show.legend = F) +
    geom_point(aes(size = prop),
               show.legend = F) +
    geom_label(aes(label = prop %>% scales::number(accuracy = .01), size = prop*50), 
               fill = "white", 
               hjust = "inward",
               show.legend = F) +
    theme_minimal() +
    labs(
      y = name
    )
}

names <- names(train %>% select(where(is.character)))
all_plots <- map(names, plot_all_full_train)
cowplot::plot_grid(plotlist = all_plots)


plot_all_full_test <- function(name = "region") {
  
  test %>% 
    count(.data[[name]]) %>% 
    mutate(prop = n/sum(n)) %>% 
    ggplot(aes(y = fct_reorder(.data[[name]], prop),
               x = prop)) +
    geom_segment(aes(xend = 0, yend = .data[[name]]),
                 show.legend = F) +
    geom_point(aes(size = prop),
               show.legend = F) +
    geom_label(aes(label = prop %>% scales::number(accuracy = .01), size = prop*50), 
               fill = "white", 
               hjust = "inward",
               show.legend = F) +
    theme_minimal() +
    labs(
      y = name
    )
}

names <- names(test %>% select(where(is.character)))
all_plots <- map(names, plot_all_full_test)
cowplot::plot_grid(plotlist = all_plots)


```

Visualizamos que em ambos os casos as proporções foram mantidas, o que é importante para generalização do modelo.

# Recipe

As etapas de pré-processamento serão feitas utilizando o pacote `recipes`. Onde cada step é responsável por uma transformação.

Optei por utilizar o one hot encoder em praticamente todas as variáveis categóricas, com excessão da `age_band` que é uma variável ordinal e pode apenas ser utilizada como fator.

```{r}
recipe_spec <- recipe(final_score_exam ~ ., data = train) %>% 
  update_role(code_module, code_presentation, id_student, new_role = "indicator") %>%
  step_string2factor(age_band, levels = c("0-35", "35-55", "55<=")) %>% 
  step_dummy(all_predictors(), -num_of_prev_attempts, -age_band, one_hot = T)


kbl(recipe_spec %>% prep() %>% juice() %>% head()) %>%
  kable_paper("striped") %>%
  scroll_box(width = "100%")
```

Explicação dos steps:

- **update_role**: cria uma especificação para algumas variáveis que não serão utilizadas como preditores mas terão um papel de identificação.
- **step_string2factor**: conversão de variáveis categóricas para fator
- **step_dummy**: aplicação do one-hot-encoding

# Model

Na etapa anterior nós criamos especificações de pré-processamento, nessa nós criaremos especificações dos modelos.

```{r}
# 1 - Random Forest
rf_spec <- rand_forest(mode = "regression") %>% 
  set_engine("ranger")

# 2 - Linear Regression
lm_spec <- linear_reg() %>% 
  set_engine("lm")
```

# Worflow

No worflow você coloca tudo junto (modelo e receita).

```{r}
# 1 - workflow random forest
wf_rf_spec <- workflow() %>% 
  add_recipe(recipe_spec) %>% 
  add_model(rf_spec)

# 2 - workflow linear regression
wf_lm_spec <- wf_rf_spec %>% update_model(lm_spec)

# objetos workflow
wf_rf_spec
wf_lm_spec
```

# Fit

No fit é onde de fato todas as especificações passadas são aplicadas e treinadas nos dados de treino.

```{r}
fitted_rf_wf <- wf_rf_spec %>% 
  fit(data = train)

fitted_lm_wf <- wf_lm_spec %>% 
  fit(data = train)

```

# Predict

Aqui vamos realizar as previsões e salvar os resultados dos dois modelos em uma nova tabela.

```{r}
results_treino <- fitted_rf_wf %>% 
  predict(new_data = train) %>% 
  mutate(valor_real = train$final_score_exam,
         model = "rf") %>% 
  bind_rows(fitted_lm_wf %>% 
              predict(new_data = train) %>% 
              mutate(valor_real = train$final_score_exam,
                     model = "lm"))

results_test <- fitted_rf_wf %>% 
  predict(new_data = test) %>% 
  mutate(valor_real = test$final_score_exam,
         model = "rf") %>% 
  bind_rows(fitted_lm_wf %>% 
              predict(new_data = test) %>% 
              mutate(valor_real = test$final_score_exam,
                     model = "lm"))


results_test %>% 
  group_by(model) %>% 
  rmse(valor_real, .pred)

results_treino %>% 
  group_by(model) %>% 
  rmse(valor_real, .pred)

```

Vemos que os dois modelos possuem valores bem próximos do RMSE, quase 19.8 e 19.9. Podemos então visualizar o resultado dos valores preditos vs valores reais.

```{r}
results_test %>%
  #filter(model == "rf") %>% 
  mutate(task = "Testing") %>%
  bind_rows(results_treino %>%
              mutate(task = "Training")) %>%
  ggplot(aes(valor_real, .pred, color = model)) +
  geom_abline(lty = 2, color = "gray80", size = 1.5) +
  geom_point(alpha = 0.5) +
  scale_color_manual(values = c("#012623", "#04BF68")) +
  facet_wrap(~task) +
  labs(
    x = "Valor Real",
    y = "Performance Predita",
    color = "Tipo de modelo",
    title = "Valores reais vs valores observados nas previsões"
  ) +
  theme_minimal()

```

Pelo gráfico dos resíduos vemos que os dois modelos tiveram resultados bem próximos, tanto nos dados de treino quanto nos dados de teste. O ideal formato desse gráfico é que os pontos estejam o mais próximo possível de uma linha reta, porém nossos resultados não foram muito bons, ainda existe muita variância para ser explicada pelo modelo.

Podemos aplicar uma técnica de resampling para investigar como o modelo se comporta em diferentes dados de validação.

```{r}
# apenas nos dados de treino
# mantendo a proporção do split inicial
set.seed(1234)
score_folds <- vfold_cv(train, strata = "highest_education", v = 5)

rf_res <- fit_resamples(
  wf_rf_spec,
  score_folds,
  control = control_resamples(save_pred = TRUE, verbose = T)
)

rf_res %>% 
  collect_metrics()

# melhor modelo
rf_res %>% 
  show_best(metric = "rmse")

```

O melhor RMSE considerando a média dos 5 conjuntos de dados de validação foi de 20.1, o que é um valor um pouco maior do que quando considerado apenas os dados de treino. A explicação é que na técnica de resampling você acaba reduzindo ainda mais os dados para ser possível treinar em uma parte e investigar a performance em outra, e como os dados já eram poucos fica mais difícil de capturar algum padrão nos dados.

# Tunning

O Random Forest é um algorítmo que possui alguns parâmetros que podem ser otimizados em cada resample no intuito de otimizar alguma métrica, essa técnica é conhecida como tunning dos parâmetros.

```{r}
# aumentando os folds para 10
set.seed(1234)
score_folds <- vfold_cv(train, strata = "highest_education", v = 10)

# tunning
model_spec_rf_tune <- rand_forest(
    mode    = "regression",
    mtry    = tune(),
    trees   = tune(),
    min_n   = tune()
) %>% 
    set_engine("ranger")


wflw_spec_rf_tune <- workflow() %>%
    add_model(model_spec_rf_tune) %>%
    add_recipe(recipe_spec)

# tuning
library(tictoc)
tic()
set.seed(123)
tune_results_rf <- wflw_spec_rf_tune %>%
    tune_grid(
        resamples = score_folds,
        grid      = 5,
        control   = control_grid(verbose = TRUE, allow_par = TRUE)
    )
toc()


kbl(tune_results_rf %>% show_best(metric = "rmse")) %>%
  kable_paper("striped") %>%
  scroll_box(width = "100%")

```
<p style="margin-bottom:40px"></p>

Acabamos aumentando os valores do RMSE. Ao final, nosso melhor modelo performou muito ruim, explicando minimamente a variância presente nos dados e com RMSE de aproximadamente 20. Realmente acredito que não tem muito a ser feito nesse modelo de regressão, pois precisamos de mais dados e mais variáveis. 

Uma alternativa que proponho é a avaliação da performance dos estudantes utilizando um modelo de classificação e a variável de `final_result` para prever a probabilidade do estudante ser aprovado ou não.
<p style="margin-bottom:40px"></p>

# Classification

Vamos ver então como se sai o modelo de classificação. Para isso, vamos precisar fazer alguns ajustes nos dados, veja o código abaixo.

```{r}
student_final_result_tbl <- student_info %>% 
  mutate(across(where(is.character), as_factor)) %>% 
  drop_na(imd_band) %>% 
  # ordem correta dos fatores
  mutate(imd_band = fct_rev(imd_band),
         age_band = fct_rev(age_band)) %>% 
  select(-studied_credits) %>% 
  mutate(imd_band = as.ordered(imd_band))

# visualizando as proporções depois do tratamento
plot_all_full_trans <- function(name = "region") {
  
  student_final_result_tbl %>% 
    count(.data[[name]]) %>% 
    mutate(prop = n/sum(n)) %>% 
    ggplot(aes(y = fct_reorder(.data[[name]], prop),
               x = prop)) +
    geom_segment(aes(xend = 0, yend = .data[[name]]),
                 show.legend = F) +
    geom_point(aes(size = prop),
               show.legend = F) +
    geom_label(aes(label = prop %>% scales::number(accuracy = .01), size = prop*50), 
               fill = "white", 
               hjust = "inward",
               show.legend = F) +
    theme_minimal() +
    labs(
      y = name
    )
}

names <- names(student_final_result_tbl %>% select(where(is.factor)))
all_plots <- map(names, plot_all_full_trans)
cowplot::plot_grid(plotlist = all_plots)


```

Classes com valores muito baixos podem gerar problemas na hora das divisões entre treino e teste, por isso iremos agrupar algumas classes segundo descrição abaixo:

- **age_band**: vai passar a ter duas variáveis categóricas, de 0 a 35 anos e acima de 35 anos.
- **highest_education**: vai passar a ter uma variável chamada "outros" oriunda da junção de `HE Qualification`, `No Formal quals` e `Post Graduate Qualification`.
- **final_result**: `Withdrawn` e `Fail` serão consideradas como `Fail` e `Distinction` e `Pass` como `Pass`.

```{r}
student_final_result_v2_tbl <- student_final_result_tbl %>% 
  mutate(age_band = as.character(age_band)) %>% 
  mutate(age_band = case_when(
    age_band == "55<=" ~ "35-55",
    TRUE ~ age_band
  )) %>% 
  mutate(age_band = case_when(
    age_band == "35-55" ~ "35<=",
    TRUE ~ age_band
  )) %>% 
  mutate(age_band = factor(age_band, levels = c("0-35", "35<="))) %>% 
  mutate(age_band = as.ordered(age_band)) %>% 
  mutate(highest_education = fct_lump(highest_education, prop = 0.2)) %>% 
  mutate(highest_education = factor(highest_education, levels = c("Other", "Lower Than A Level", "A Level or Equivalent"))) %>% 
  mutate(highest_education = as.ordered(highest_education)) %>% 
  mutate(final_result = final_result %>% as.character()) %>% 
  mutate(final_result = case_when(
    final_result == "Withdrawn" ~ "Fail",
    final_result == "Distinction" ~ "Pass",
    TRUE ~ final_result
  )) %>% 
  mutate(final_result = as.factor(final_result))

kbl(student_final_result_v2_tbl %>% head()) %>%
  kable_paper("striped") %>%
  scroll_box(width = "100%")

```
<p style="margin-bottom:40px"></p>

Vamos visualizar nossas variáveis categóricas mais uma vez antes da modelagem.

```{r}
# visualizando as proporções
plot_all_full_prepro <- function(name = "region") {
  
  student_final_result_v2_tbl %>% 
    count(.data[[name]]) %>% 
    mutate(prop = n/sum(n)) %>% 
    ggplot(aes(y = fct_reorder(.data[[name]], prop),
               x = prop)) +
    geom_segment(aes(xend = 0, yend = .data[[name]]),
                 show.legend = F) +
    geom_point(aes(size = prop),
               show.legend = F) +
    geom_label(aes(label = prop %>% scales::number(accuracy = .01), size = prop*50), 
               fill = "white", 
               hjust = "inward",
               show.legend = F) +
    theme_minimal() +
    labs(
      y = name
    )
}

names <- names(student_final_result_v2_tbl %>% select(where(is.factor)))
all_plots <- map(names, plot_all_full_prepro)
cowplot::plot_grid(plotlist = all_plots)

```

Pronto, nosso problema agora é uma classificação binomial e pronta modelagem. Vamos iniciar com os splits entre treino e teste.

```{r}
# split train and test
set.seed(1234)
splits <- initial_split(student_final_result_v2_tbl, prop = .7, strata = final_result)

train <- training(splits)
test <- testing(splits)

# recipe random forest
recipe_class_rf_spec <- recipe(final_result ~ ., data = train) %>% 
  update_role(code_presentation, id_student, new_role = "Identification") %>%
  step_dummy(all_predictors(), -num_of_prev_attempts, -age_band, -imd_band, -highest_education)

# dataset para o random forest  
kbl(recipe_class_rf_spec %>% prep() %>% juice() %>% head()) %>%
  kable_paper("striped") %>%
  scroll_box(width = "100%")

# recipe log regression
recipe_class_logreg_spec <- recipe(final_result ~ ., data = train) %>% 
  update_role(code_presentation, id_student, new_role = "Identification") %>%
  step_ordinalscore(highest_education, imd_band, age_band) %>% 
  step_dummy(all_predictors(), -num_of_prev_attempts, -age_band, -imd_band, -highest_education)

# dataset para o log regression
kbl(recipe_class_logreg_spec %>% prep() %>% juice() %>% head()) %>%
  kable_paper("striped") %>%
  scroll_box(width = "100%")
```

Vamos agora a criação dos workflows, muito similar ao que fizemos anteriormente.

```{r}
# models
model_lr <- logistic_reg(mode = "classification") %>% 
  set_engine("glm")

# Os parâmetros escolhidos no random forest 
# foram obtidos de uma etapa prévia de tunning
model_rf <- rand_forest() %>% 
  set_engine("ranger", 
             importance = "impurity") %>%  # variable importance
  set_mode("classification")

# workflow
lr_wflow <- workflow() %>% 
  add_recipe(recipe_class_logreg_spec) %>% 
  add_model(model_lr)

rf_wflow <- workflow() %>% 
  add_recipe(recipe_class_rf_spec) %>%
  add_model(model_rf) 

```

Com tudo organizado, agora podemos seguir para treino e predição dos modelos.

```{r}
# treino
# treino do modelo com logistic regression
model_log_fit <- lr_wflow %>% 
  fit(data = train)

# treino do modelo com random forest
set.seed(123)
model_rf_fit <- rf_wflow %>% 
  fit(data = train)

# predições com o modelo treinado de logistic regression
pred_lr <- predict(model_log_fit, test)
pbp_pred_lr <- pred_lr %>% 
  bind_cols(test %>% select(final_result)) %>% 
  # adicionando coluna com as probabilidades
  bind_cols(predict(model_log_fit, test, type = "prob"))

# predições com o modelo treinado de random forest
pred_rf <- predict(model_rf_fit, test)
pbp_pred_rf <- pred_rf %>% 
  bind_cols(test %>% select(final_result)) %>% 
  # adicionando coluna com as probabilidades
  bind_cols(predict(model_rf_fit, test, type = "prob"))
```

A saída de cada predição pode ser vista abaixo.

```{r}
# logistic regression
kbl(pbp_pred_lr %>% head()) %>%
  kable_paper("striped") %>%
  scroll_box(width = "100%")

# random forest
kbl(pbp_pred_rf %>% head()) %>%
  kable_paper("striped") %>%
  scroll_box(width = "100%")

```

E por fim avaliação das nossas métricas.

```{r}
# Random Forest -------------------------------------------------------
# acurácia e kap
pbp_pred_rf %>% 
  metrics(truth = final_result, 
          .pred_class)

# confusion matrix
pbp_pred_rf %>% 
  conf_mat(final_result, .pred_class)

# Log reg ----------------------
# acurácia e kap
pbp_pred_lr %>% 
  metrics(truth = final_result, 
          .pred_class)

# confusion matrix
pbp_pred_lr %>% 
  conf_mat(final_result, .pred_class)

```

Nesse momento nós temos duas informações importantes sobre nossos modelos, acurácia e confusion matrix. A acurácia do random forest é alguns pontos maior que a do logistic regression, porém o modelo de logistic regression se saiu melhor em identificar estudantes que seriam reprovados (Fail).

Eu prefiro analisar uma outra métrica a ROC AUC, pois é uma métrica mais robusta quando comparada as outras.

Visualizando a curva ROC para os dois modelos.

```{r}
# lr roc auc
roc_auc_lr <- pbp_pred_lr %>%
  roc_curve(truth = final_result,
          .pred_Fail) %>% 
  mutate(model = "Logistic Regression")

# rf roc auc
roc_auc_rf <- pbp_pred_rf %>%
  roc_curve(truth = final_result,
            .pred_Fail)%>% 
  mutate(model = "Random Forest")

geral_roc_auc <- bind_rows(roc_auc_lr, roc_auc_rf)

geral_roc_auc %>%
  ggplot(aes(x = 1 - specificity,
             y = sensitivity,
             color = model)) +
  geom_line(size = 1.5, alpha = .7) +
    geom_abline(
    lty = 2, alpha = 0.5,
    color = "gray50",
    size = .8
  ) +
  scale_color_manual(values = c("#329429", "#23664B")) +
  theme_minimal() +
  labs(
    title = "Curva ROC dos dois modelos utilizados"
  )

```

Assim vemos que o random forest se saiu melhor, pois possui uma área maior abaixo da curva.

Nesse ponto eu fiz mais um tunning, dessa vez para o xgboost e random forest, como o procedimento levou um certo tempo, eu preferir deixar o código comentado e salvei os modelos já treinados, que você pode encontrar na pasta do projeto.

```{r}
# estimate perfomance with cross validation
# set.seed(123)
# score_fold <- vfold_cv(train, v = 5, strata = final_result)
# 

# xgboost -------------------------------------------------------
# tune_xgb <- boost_tree(
#   trees = 1000,
#   tree_depth = tune(), min_n = tune(),
#   loss_reduction = tune(),                     ## first three: model complexity
#   sample_size = tune(), mtry = tune(),         ## randomness
#   learn_rate = tune(),                         ## step size
# ) %>%
#   set_engine("xgboost") %>%
#   set_mode("classification")
# 
# 
# # Tunning Workflow
# 
# tune_xgb_wflw <- workflow() %>%
#   add_model(tune_xgb) %>%
#   add_recipe(recipe_class_logreg_spec)
# 
# 
# # Tunning grid
# 
# xgb_grid <- grid_latin_hypercube(
#   tree_depth(),
#   min_n(),
#   loss_reduction(),
#   sample_size = sample_prop(),
#   finalize(mtry(), train),
#   learn_rate(),
#   size = 30
# )
# 
# tictoc::tic()
# tune_res <- tune_grid(
#   tune_xgb_wflw,
#   resamples = score_fold,
#   grid = xgb_grid, # 15 combos of model parameters
#   control = control_grid(verbose = TRUE, save_pred = TRUE),
#   metrics = metric_set(accuracy, kap, roc_auc)
# )
# tictoc::toc()
# 
# best_fit_auc <- select_best(tune_res, "roc_auc")
# 
# tune_res %>% show_best(metric ="roc_auc")
# 
# # Finalizando com o melhor modelo do tunning
# final_xgb <- finalize_workflow(
#   tune_xgb_wflw,
#   best_fit_auc
# )
# 
# # treino
# # treino do modelo com logistic regression
# model_xgb_best_fit <- final_xgb %>% 
#   fit(data = train)
# 
# saveRDS(model_xgb_best_fit, "model_xgb_best_fit.rds")


# random forest ------------------------------------------
# rf_tuning_spec <- rand_forest(
#   mtry = tune(),
#   trees = 1000,
#   min_n = tune()
# ) %>%
#   set_mode("classification") %>%
#   set_engine("ranger")
# # 
# tune_rf_wf <- workflow() %>%
#   add_model(rf_tuning_spec) %>%
#   add_recipe(recipe_class_spec)
# 
# tictoc::tic()
# tune_res <- tune_grid(
#   tune_rf_wf,
#   resamples = score_fold,
#   grid = 10, # 15 combos of model parameters
#   control = control_grid(verbose = TRUE, save_pred = TRUE),
#   metrics = metric_set(accuracy, kap, roc_auc)
# )
# tictoc::toc()
# 
# best_fit_auc <- select_best(tune_res, "roc_auc")
# 
# tune_res %>% show_best(metric ="roc_auc")
# 
# # Finalizando com o melhor modelo do tunning
# final_rf <- finalize_workflow(
#   tune_rf_wf,
#   best_fit_auc
# )
# 
# saveRDS(final_rf, "final_rf.rds")

```



```{r}
model_xgb_best_fit <- readRDS("model_xgb_best_fit.rds")

# predições com o modelo treinado de logistic regression
pred_xgb_tuned <- predict(model_xgb_best_fit, test)
pbp_pred_xg <- pred_xgb_tuned %>% 
  bind_cols(test %>% select(final_result)) %>% 
  # adicionando coluna com as probabilidades
  bind_cols(predict(model_xgb_best_fit, test, type = "prob"))

# xgb -------------------------------------------------------
# acurácia e kap
pbp_pred_xg %>% 
  metrics(truth = final_result, 
          .pred_class)

# confusion matrix
pbp_pred_xg %>% 
  conf_mat(final_result, .pred_class)


# random forest -------------------------------------------
final_rf <- readRDS("final_rf.rds")

model_rf_best_fit <- final_rf %>%
  fit(data = train)

pred_rf <- predict(model_rf_best_fit, test)
pbp_pred_rf <- pred_rf %>%
  bind_cols(test %>% select(final_result)) %>%
  bind_cols(predict(model_rf_best_fit, test, type = "prob"))

# acurácia e kap
pbp_pred_rf %>%
  metrics(truth = final_result,
          .pred_class)

# confusion matrix
pbp_pred_rf %>%
  conf_mat(final_result, .pred_class)

```

```{r}
# roc_auc
roc_auc_xg_tuned <- pbp_pred_xg %>%
  roc_curve(truth = final_result,
          .pred_Fail) %>% 
  mutate(model = "Xgboost - Tunned")

# Visualizando a roc_curve
roc_auc_rf_tuned <- pbp_pred_rf %>%
  roc_curve(truth = final_result,
            .pred_Fail) %>% 
  mutate(model = "Random Forest - Tunned")

geral_roc_auc <- bind_rows(roc_auc_lr, roc_auc_rf, roc_auc_xg_tuned, roc_auc_rf_tuned)

geral_roc_auc %>%
  ggplot(aes(x = 1 - specificity,
             y = sensitivity,
             color = model)) +
  geom_line(size = 1.5, alpha = .7) +
    geom_abline(
    lty = 2, alpha = 0.5,
    color = "gray50",
    size = .8
  ) +
  scale_color_manual(values = c("#329429", "#23664B", "#1BA673", "#8C520A")) +
  theme_minimal() +
  labs(
    title = "Curva ROC dos dois modelos utilizados"
  )

# roc_auc all models
roc_auc_lr_values <- pbp_pred_lr %>%
  roc_auc(truth = final_result,
          .pred_Fail) %>% 
  mutate(model = "Logistic Regression")

roc_auc_rf_values <- pbp_pred_rf %>%
  roc_auc(truth = final_result,
            .pred_Fail)%>% 
  mutate(model = "Random Forest")

roc_auc_xg_tuned_values <- pbp_pred_xg %>%
  roc_auc(truth = final_result,
          .pred_Fail) %>% 
  mutate(model = "Xgboost - Tunned")

roc_auc_rf_tuned_values <- pbp_pred_rf %>%
  roc_auc(truth = final_result,
            .pred_Fail) %>% 
  mutate(model = "Random Forest - Tunned")

geral_roc_auc_values <- bind_rows(roc_auc_lr_values, roc_auc_rf_values, roc_auc_xg_tuned_values, roc_auc_rf_tuned_values) %>% arrange(desc(.estimate))

# tabela com todos os quatro modelos treinados
kbl(geral_roc_auc_values) %>%
  kable_paper("striped") %>%
  scroll_box(width = "100%")

```

O que podemos ver, tanto pelo gráfico quanto pelas estimativas é que o xgboost performou melhor que os outros modelos quando avaliado nos dados de teste, e certamente será nossa escolha.

Por fim, eu trago aqui um plot de importância de variáveis, por ser um entendimento interessante para o negócio.

```{r}
library(vip)
# logistic_regression
model_xgb_best_fit %>% 
  pull_workflow_fit() %>% 
  vip(num_features = 10) +
  geom_col(fill = "#23664B") +
  theme_minimal() +
  labs(
    title = "Variáveis mais importantes \npara o modelo de XGboost tunado"
  )

```

Veja que o top 3 de variáveis mais importantes são:

1. `highest_education` 
1. `imd_band`
1. `num_of_prev_attempts`

Ambas relacionadas com o nível educacional prévio do estudante, nível de carência da região em que mora e se o mesmo é ou não repetente naquele curso.